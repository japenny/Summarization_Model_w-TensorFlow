{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbbUOZQuVm7V"
      },
      "source": [
        "# Abstractive LSTM Text Summarization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXamseOBiRnz"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtEIWIDAzGKh"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow-cloud\n",
        "!pip install tensorflow_cloud\n",
        "# !pip install tensorflow-addons\n",
        "# !pip install keras-nlp\n",
        "!pip install rouge-score\n",
        "# !pip install rouge\n",
        "!pip install tf-nightly\n",
        "!pip install pyarrow\n",
        "!pip install pyarrow==8.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_cloud"
      ],
      "metadata": {
        "id": "tcw64b9TL3zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlLkpAXYwNmx",
        "outputId": "6dda2d1e-c689-4bf0-b2df-59e63f7e9645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b028gw4xSNYL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "66d52e88-c3a0-4206-8be4-4b009b3c5277"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6ffd1652a3b9>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\"\"\" Building model \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# from tensorflow.python import keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# from tensorflow.python.layers import layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muser_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/saved_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/builder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_SavedModelBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSavedModelBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/builder_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaved_model_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaver_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0m_np_bfloat16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0m_np_float8_e4m3fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat8_e4m3fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0m_np_float8_e5m2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat8_e5m2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle"
          ]
        }
      ],
      "source": [
        "\"\"\" Datasets \"\"\"\n",
        "import pyarrow\n",
        "from datasets import load_dataset, load_dataset_builder\n",
        "# import gensim.downloader as api\n",
        "\n",
        "\"\"\" Building model \"\"\"\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from tensorflow.keras.layers import Attention, Bidirectional, Concatenate, Dense, Embedding, Flatten, Input, LayerNormalization, LSTM, MultiHeadAttention\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
        "from tensorflow.keras.regularizers import *\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "\"\"\" Training/Testing model \"\"\"\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.initializers import Zeros\n",
        "from tensorflow.keras.metrics import Metric, F1Score\n",
        "# from keras_nlp.metrics import RougeL\n",
        "from rouge_score import rouge_scorer as rs\n",
        "# from rouge import Rouge\n",
        "# from tensorflow_addons.seq2seq import BeamSearchDecoder\n",
        "# from keras_nlp.utils import beam_search\n",
        "\n",
        "\"\"\" TF Cloud Training \"\"\"\n",
        "import tensorflow_cloud as tfc\n",
        "from tensorflow_cloud.core.docker_config import DockerConfig\n",
        "\n",
        "\"\"\" Data processing/visualization \"\"\"\n",
        "# import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\"\"\" Cloud \"\"\"\n",
        "from google.colab import auth, files\n",
        "from google.cloud import storage\n",
        "\n",
        "\"\"\" Other \"\"\"\n",
        "import sys\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr5Ja47mNUIZ"
      },
      "source": [
        "## Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4GrqJCRbOvs"
      },
      "outputs": [],
      "source": [
        "\"\"\" Check Data \"\"\"\n",
        "ds_name = 'cnn_dailymail' # 'GEM/wiki_lingua' 'ccdv/pubmed-summarization' 'scientific_papers'\n",
        "ds_sub = '3.0.0'\n",
        "builder = load_dataset_builder(ds_name, ds_sub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3IO0630B7Ut"
      },
      "outputs": [],
      "source": [
        "\"\"\" Check Data cont. \"\"\"\n",
        "print(builder.info.description)\n",
        "builder.info.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLhN-tKz3eeu"
      },
      "outputs": [],
      "source": [
        "\"\"\" Load Dataset \"\"\"\n",
        "dataset = load_dataset(ds_name, ds_sub)\n",
        "\n",
        "split_train = len(dataset['train']['article'])\n",
        "split_val = len(dataset['validation']['article'])\n",
        "\n",
        "if not tfc.remote():\n",
        "  split_train = 1000\n",
        "  split_val = 1000\n",
        "\n",
        "st = 30\n",
        "pre_auth = False\n",
        "co_model = 7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preperation"
      ],
      "metadata": {
        "id": "rFcdMS9voycV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - Split Data"
      ],
      "metadata": {
        "id": "0EI4TrajcxEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gdw8nQ-IKS_c"
      },
      "outputs": [],
      "source": [
        "\"\"\" Split Data and add sos/eos tokens \"\"\"\n",
        "sos_token = '<sos>'\n",
        "eos_token = '<eos>'\n",
        "\n",
        "x_train = np.array([f'{sos_token} {art} {eos_token}' for art in dataset['train']['article'][:split_train]])\n",
        "time.sleep(st)\n",
        "y_train = np.array([f'{sos_token} {sum} {eos_token}' for sum in dataset['train']['highlights'][:split_train]])\n",
        "time.sleep(st)\n",
        "\n",
        "x_val = np.array([f'{sos_token} {art} {eos_token}' for art in dataset['validation']['article'][:split_val]])\n",
        "y_val = np.array([f'{sos_token} {sum} {eos_token}' for sum in dataset['validation']['highlights'][:split_val]])\n",
        "\n",
        "time.sleep(st)\n",
        "x_train = np.concatenate([x_train, x_val], axis=0)\n",
        "time.sleep(st)\n",
        "y_train = np.concatenate([y_train, y_val], axis=0)\n",
        "\n",
        "del x_val, y_val\n",
        "\n",
        "x_test = np.array([f'{sos_token} {art} {eos_token}' for art in dataset['test']['article']])\n",
        "y_test = np.array([f'{sos_token} {sum} {eos_token}' for sum in dataset['test']['highlights']])\n",
        "\n",
        "print(f'x_train shape: {x_train.shape}, y_train shape: {y_train.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnGHfGZpKnn4"
      },
      "outputs": [],
      "source": [
        "\"\"\" Data Samples \"\"\"\n",
        "df = pd.DataFrame({'Article':x_train[:5], 'Summary':y_train[:5]})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td7fhHRbZwQt"
      },
      "outputs": [],
      "source": [
        "del dataset, df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyZafR4RfufJ"
      },
      "source": [
        "### - Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQkT_FGdftGG"
      },
      "outputs": [],
      "source": [
        "### optimize preprocessing ###\n",
        "\"\"\"\n",
        "tokenize data first\n",
        "split data\n",
        "\n",
        "get all stop word indexes from tokenizer.word_index\n",
        "remove stop stop words??\n",
        "\n",
        "pad data as normal\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhAueUfbflCS"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['cnn', 'reuters'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrDwxDFwfzuw"
      },
      "outputs": [],
      "source": [
        "\"\"\" Lowercase all words \"\"\"\n",
        "x_train, y_train = np.char.lower(x_train), np.char.lower(y_train)\n",
        "x_test, y_test = np.char.lower(x_test), np.char.lower(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOA-p_ybf2tD"
      },
      "outputs": [],
      "source": [
        "\"\"\" Remove stop words \"\"\"\n",
        "pattern = re.compile(\"\\\\b(\" + \"|\".join(stop_words) + \")\\\\b\")\n",
        "\n",
        "vec_pattern = np.vectorize(lambda text:pattern.sub('', text))\n",
        "time.sleep(st)\n",
        "x_train = vec_pattern(x_train)\n",
        "time.sleep(st)\n",
        "y_train = vec_pattern(y_train)\n",
        "\n",
        "del pattern, stop_words, vec_pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - Transform Data"
      ],
      "metadata": {
        "id": "RevvLw3IbyiD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdS-HHpFgAGp"
      },
      "outputs": [],
      "source": [
        "\"\"\" Tokenize data \"\"\"\n",
        "tokenizer = Tokenizer(filters='\"#$%&(!?.)\\'*+,-/:;=@[\\\\]^_`{|}~\\t\\n')#, oov_token=\"<unk>\")\n",
        "\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "tokenizer.fit_on_texts(y_train)\n",
        "\n",
        "x_train, y_train = tokenizer.texts_to_sequences(x_train), tokenizer.texts_to_sequences(y_train)\n",
        "x_test, y_test = tokenizer.texts_to_sequences(x_test), tokenizer.texts_to_sequences(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqEzkUGnbS0F"
      },
      "outputs": [],
      "source": [
        "\"\"\" Pad tokenized data \"\"\"\n",
        "x_train, y_train = pad_sequences(x_train, padding='post'), pad_sequences(y_train, padding='post')\n",
        "x_test, y_test = pad_sequences(x_test, padding='post'), pad_sequences(y_test, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLx5AJZzbXcj"
      },
      "outputs": [],
      "source": [
        "\"\"\" After Processing Data Samples \"\"\"\n",
        "df = pd.DataFrame(x_train[:3])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g8bf808-pQ-"
      },
      "outputs": [],
      "source": [
        "del df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model\n"
      ],
      "metadata": {
        "id": "_gqHbB2OoKjz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2YrkK0mM0W2"
      },
      "source": [
        "### - Develop Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Load Pre-Trained Word Embeddings \"\"\"\n",
        "w2v_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCt3YEPv374K",
        "outputId": "fb264edf-a0f2-417f-bff3-872fb0898eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7PqZMOf6JVz"
      },
      "outputs": [],
      "source": [
        "\"\"\" Create Embedding Matrix \"\"\"\n",
        "vocab_dim = len(tokenizer.word_index)+1\n",
        "emb_dim = 300\n",
        "x_row, x_col = x_train.shape\n",
        "y_row, y_col = y_train.shape\n",
        "\n",
        "emb_matrix = np.zeros((vocab_dim, emb_dim))\n",
        "\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "  if word in w2v_model:\n",
        "    emb_matrix[idx] = w2v_model[word]\n",
        "\n",
        "del w2v_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiNL6JsDbhJs"
      },
      "outputs": [],
      "source": [
        "\"\"\" Develop Model \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "################ CREATING INITIAL STATES OF LSTM ##################\n",
        "# Define the input layer\n",
        "\n",
        "input_shape = (10, 20)  # 10 timesteps, 20 features\n",
        "lstm_units = 128\n",
        "\n",
        "inputs = Input(shape=input_shape)\n",
        "\n",
        "# Define the LSTM layer with initial states\n",
        "lstm_layer = LSTM(units=lstm_units, return_sequences=False, return_state=False)\n",
        "\n",
        "# Specify the initial states\n",
        "initial_cell_state = np.zeros((1, lstm_units))\n",
        "initial_hidden_state = np.zeros((1, lstm_units))\n",
        "initial_state = [initial_cell_state, initial_hidden_state]\n",
        "\n",
        "# Call the LSTM layer with initial states\n",
        "outputs, final_cell_state, final_hidden_state = lstm_layer(inputs, initial_state=initial_state)\n",
        "#####################################\n",
        "  Beam Search\n",
        "  decoder_beam = BeamSearchDecoder(cell=decoder_att_out.cell,\n",
        "                                   beam_width=beam_width,\n",
        "                                   embedding_fn=decoder_emb_layer,\n",
        "                                   output_layer=decoder_dense) #decoder_lstm.cell\n",
        "\n",
        "  decoder_beam = beam_search(token_probability_fn,\n",
        "                             prompt=\n",
        "                             max_length=\n",
        "                             num_beams=\n",
        "                             )\n",
        "##################\n",
        "hierarchal attention mechanism\n",
        "reinforcement learing training\n",
        "multi-task learning framework\n",
        "##################\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(lat_0=128, lat_1=128,\n",
        "                  dr_0=0.35, dr_1=0.35, dr_2=0.35,\n",
        "                  att_0=2, att_1=2,\n",
        "                  beam_width=4,\n",
        "                  vocab_len=vocab_dim, emb_len=emb_dim):\n",
        "\n",
        "  # Encoder\n",
        "  encoder_input     = Input(shape=(None,), name='Input_0')\n",
        "\n",
        "  encoder_emb_layer = Embedding(weights=[emb_matrix], mask_zero=True, trainable=False, name='Embedding_0')\n",
        "  encoder_emb_input = encoder_emb_layer(encoder_input)\n",
        "\n",
        "  encoder_lstm_0    = Bidirectional(LSTM(units=lat_0, dropout=dr_0, return_sequences=True), name='Bidirectional_LSTM_0')\n",
        "  encoder_lstm_out  = encoder_lstm_0(encoder_emb_input)\n",
        "\n",
        "  encoder_lay_norm  = LayerNormalization(name='Layer_Norm_0')\n",
        "  encoder_lstm_norm = encoder_lay_norm(encoder_lstm_out)\n",
        "\n",
        "  encoder_self_att  = MultiHeadAttention(num_heads=att_0, key_dim=emb_len, query_key_shared=True, name='Attention_0')\n",
        "  encoder_att_out   = encoder_self_att(encoder_lstm_norm)\n",
        "\n",
        "  encoder_lstm_1    = Bidirectional(LSTM(units=lat_1, dropout=dr_1, return_sequences=True, return_state=True), name='Bidirectional_LSTM_1')\n",
        "  encoder_out, forward_h, forward_c, backward_h, backward_c = encoder_lstm_1(encoder_att_out)\n",
        "\n",
        "  forward_h_concat  = Concatenate(name='Concat_0')([forward_h, backward_h])\n",
        "  forward_c_concat  = Concatenate(name='Concat_1')([forward_c, backward_c])\n",
        "  encoder_states    = [forward_h_concat, forward_c_concat]\n",
        "\n",
        "\n",
        "  # Decoder\n",
        "  decoder_input     = Input(shape=(None,), name='Input_1')\n",
        "\n",
        "  decoder_emb_layer = Embedding(weights=[emb_matrix], mask_zero=True, trainable=False, name='Embedding_1')\n",
        "  decoder_emb_out   = decoder_emb_layer(decoder_input)\n",
        "\n",
        "  decoder_lstm      = LSTM(units=lat_1*2, dropout=dr_2, return_sequences=True, name='LSTM_0')\n",
        "  decoder_lstm_out  = decoder_lstm(decoder_emb_out, initial_state=encoder_states)\n",
        "\n",
        "  decoder_lay_norm  = LayerNormalization(name='Layer_Norm_1')\n",
        "  decoder_lstm_norm = decoder_lay_norm(decoder_lstm_out)\n",
        "\n",
        "  decoder_multi_att = MultiHeadAttention(num_heads=att_1, key_dim=emb_len, name='Attention_1')\n",
        "  decoder_att_out   = decoder_multi_att(query=decoder_lstm_norm, key=encoder_out)\n",
        "\n",
        "  decoder_dense     = Dense(vocab_len, activation='softmax', name='Dense_0')\n",
        "  decoder_dense_out = decoder_dense(decoder_att_out)\n",
        "\n",
        "  # Create model\n",
        "  model = Model([encoder_input.input, decoder_input(encoder_input.output)], decoder_dense_out, name='Text_Summarization_Model')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "VMz9j-YsRs1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_model(lat_0=128, lat_1=128,\n",
        "                  dr_0=0.35, dr_1=0.35, dr_2=0.35,\n",
        "                  att_0=2, att_1=2,\n",
        "                  beam_width=4,\n",
        "                  vocab_len=vocab_dim, emb_len=emb_dim):\n",
        "\n",
        "  # Encoder\n",
        "  encoder_input     = Input(shape=(None,), name='Input_0')\n",
        "\n",
        "  encoder_emb_layer = Embedding(weights=[emb_matrix], mask_zero=True, trainable=False, name='Embedding_0')\n",
        "  encoder_emb_input = encoder_emb_layer(encoder_input)\n",
        "\n",
        "  encoder_lstm_0    = Bidirectional(LSTM(units=lat_0, dropout=dr_0, return_sequences=True), name='Bidirectional_LSTM_0')\n",
        "  encoder_lstm_out  = encoder_lstm_0(encoder_emb_input)\n",
        "\n",
        "  encoder_lay_norm  = LayerNormalization(name='Layer_Norm_0')\n",
        "  encoder_lstm_norm = encoder_lay_norm(encoder_lstm_out)\n",
        "\n",
        "  encoder_att       = MultiHeadAttention(num_heads=att_0, key_dim=emb_len, name='Attention_0')\n",
        "  encoder_att_out   = encoder_att(encoder_lstm_norm, encoder_lstm_norm)\n",
        "\n",
        "  encoder_lstm_1    = Bidirectional(LSTM(units=lat_1, dropout=dr_1, return_sequences=True, return_state=True), name='Bidirectional_LSTM_1')\n",
        "  encoder_out, forward_h, forward_c, backward_h, backward_c = encoder_lstm_1(encoder_att_out)\n",
        "\n",
        "  encoder_lay_norm  = LayerNormalization(name='Layer_Norm_1')\n",
        "  encoder_lstm_norm_1 = encoder_lay_norm(encoder_out)\n",
        "\n",
        "  encoder_att_1     = MultiHeadAttention(num_heads=att_0, key_dim=emb_len, name='Attention_1')\n",
        "  encoder_att_out_1 = encoder_att_1(encoder_lstm_norm_1, encoder_lstm_norm_1)\n",
        "\n",
        "  forward_h_concat  = Concatenate(name='Concat_0')([forward_h, backward_h])\n",
        "  forward_c_concat  = Concatenate(name='Concat_1')([forward_c, backward_c])\n",
        "  encoder_states    = [forward_h_concat, forward_c_concat]\n",
        "\n",
        "\n",
        "  # Decoder\n",
        "  decoder_input = Input(shape=(None,), name='Input_1')\n",
        "\n",
        "  decoder_emb_layer = Embedding(weights=[emb_matrix], mask_zero=True, trainable=False, name='Embedding_1')\n",
        "  decoder_emb_out = decoder_emb_layer(decoder_input)\n",
        "\n",
        "  decoder_lstm = LSTM(units=lat_1*2, dropout=dr_2, return_sequences=True, name='LSTM_0')\n",
        "  decoder_lstm_out = decoder_lstm(decoder_emb_out, initial_state=encoder_states)\n",
        "\n",
        "  decoder_lay_norm = LayerNormalization(name='Layer_Norm_2')\n",
        "  decoder_lstm_norm = decoder_lay_norm(decoder_lstm_out)\n",
        "\n",
        "  decoder_multi_att = MultiHeadAttention(num_heads=att_1, key_dim=emb_len, name='Attention_2')\n",
        "  decoder_att_out = decoder_multi_att(query=decoder_lstm_norm, value=encoder_att_out_1)\n",
        "\n",
        "  decoder_lstm_1 = LSTM(units=lat_1*2, dropout=dr_2, return_sequences=True, name='LSTM_1')\n",
        "  decoder_lstm_out_1 = decoder_lstm_1(decoder_att_out)\n",
        "\n",
        "  decoder_lay_norm_1 = LayerNormalization(name='Layer_Norm_3')\n",
        "  decoder_lstm_norm_1 = decoder_lay_norm_1(decoder_lstm_out_1)\n",
        "\n",
        "  decoder_multi_att_1 = MultiHeadAttention(num_heads=att_1, key_dim=emb_len, name='Attention_3')\n",
        "  decoder_att_out_1 = decoder_multi_att_1(query=decoder_lstm_norm_1, value=encoder_att_out_1)\n",
        "\n",
        "  decoder_dense = Dense(vocab_len, activation='softmax', name='Dense_0')\n",
        "  decoder_dense_out = decoder_dense(decoder_att_out_1)\n",
        "\n",
        "  # Create model\n",
        "  model = Model([encoder_input, decoder_input], decoder_dense_out, name='Text_Summarization_Model')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "kxpS5mrDC-JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - Develop Metrics/Callbacks"
      ],
      "metadata": {
        "id": "f6NvVO4qbCqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Create rouge score metric \"\"\"\n",
        "class RougeMetric(Metric):\n",
        "\n",
        "  def __init__(self, method='avg'):\n",
        "    super().__init__(name='f1_rs')\n",
        "\n",
        "    if method not in {'avg', 'min', 'max'}:\n",
        "      raise ValueError(\"Invalid score method, expected 'min', 'avg' or 'max' (str)\")\n",
        "    self.method = method\n",
        "    self.rouge_scoring = rs.RougeScorer(['rougeL'])\n",
        "\n",
        "    if self.method == 'min':\n",
        "      self.f1_score = tf.Variable(1.0, dtype=tf.float32, trainable=False)\n",
        "    else:\n",
        "      self.f1_score = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
        "    self.co = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
        "\n",
        "  def sequences_to_texts(self, sequence): # (47, ) -> (1, )\n",
        "      return tokenizer.sequences_to_texts(sequence.numpy().reshape(1, -1))\n",
        "\n",
        "  def tf_sequences_to_texts(self, sequence): # (47, ) -> (1, )\n",
        "      return tf.py_function(self.sequences_to_texts, [sequence], tf.string)\n",
        "\n",
        "  def get_f1(self, ref, hyp): # (2, ) -> int\n",
        "      score = self.rouge_scoring.score(ref.numpy(), hyp.numpy())\n",
        "      return score['rougeL'].fmeasure\n",
        "\n",
        "  def get_rouge(self, vals): # (2, ) -> int\n",
        "      return tf.py_function(self.get_f1, [vals[0], vals[1]], tf.float32)\n",
        "\n",
        "  def update_state(self, y_true, y_preds, sample_weight=None):\n",
        "    max_preds = tf.convert_to_tensor(tf.argmax(y_preds, axis=-1)) # (50, 47)\n",
        "\n",
        "    text_preds = tf.map_fn(self.tf_sequences_to_texts, max_preds, dtype=tf.string) # (50, 47) -> (50, )\n",
        "    text_true = tf.map_fn(self.tf_sequences_to_texts, y_true, dtype=tf.string) # (50, 47) -> (50, )\n",
        "\n",
        "    scores_f1 = tf.map_fn(self.get_rouge, (text_true, text_preds), dtype=tf.float32) # (50, 50) -> (50, )\n",
        "\n",
        "    if self.method == 'min':\n",
        "      self.f1_score.assign(tf.minimum(self.f1_score, tf.reduce_min(scores_f1)))\n",
        "    elif self.method == 'avg':\n",
        "      self.f1_score.assign_add(tf.reduce_sum(scores_f1))\n",
        "      self.co.assign_add(tf.cast(tf.shape(y_true)[0], dtype=tf.float32))\n",
        "    elif self.method == 'max':\n",
        "      self.f1_score.assign(tf.maximum(self.f1_score, tf.reduce_max(scores_f1)))\n",
        "    else:\n",
        "      raise ValueError(\"Invalid score method when updating f1_score, expected 'min', 'avg' or 'max' (str)\")\n",
        "\n",
        "  def result(self):\n",
        "    if self.method == 'avg':\n",
        "      avg = self.f1_score / self.co\n",
        "      return tf.round(avg * 10_000) / 10_000\n",
        "    else:\n",
        "      return tf.round(self.f1_score * 10_000) / 10_000\n",
        "\n",
        "  def reset_state(self):\n",
        "    if self.method == 'min':\n",
        "        self.f1_score.assign(1.0)\n",
        "    else:\n",
        "        self.f1_score.assign(0.0)\n",
        "    self.co.assign(0.0)\n"
      ],
      "metadata": {
        "id": "ujZp9Fv5RbH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUziV7LIblNr"
      },
      "outputs": [],
      "source": [
        "\"\"\" Create Callbacks \"\"\"\n",
        "def get_callbacks(tb_path, cp_path,\n",
        "                  rlr_factor=0.1, rlr_patience=3,\n",
        "                  es_patience=6):\n",
        "                  #val_loss, val_f1_rs\n",
        "  early_stop   = EarlyStopping(monitor='loss',\n",
        "                               patience=es_patience)\n",
        "\n",
        "  reduce_lr    = ReduceLROnPlateau(factor=rlr_factor,\n",
        "                                   patience=rlr_patience)\n",
        "\n",
        "  tensor_board = TensorBoard(log_dir=tb_path)\n",
        "\n",
        "  model_cp     = ModelCheckpoint(filepath=cp_path,\n",
        "                                 monitor='loss',\n",
        "                                 save_best_only=True,\n",
        "                                 save_freq='epoch',\n",
        "                                 verbose=1)\n",
        "\n",
        "  return early_stop, model_cp, tensor_board#, reduce_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vp_7sh8bpiH"
      },
      "outputs": [],
      "source": [
        "\"\"\" Compile Model \"\"\"\n",
        "def compile_model(model, lr=0.001, rm_metric='avg'):\n",
        "\n",
        "  model.compile(optimizer=Adam(lr),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=[RougeMetric(rm_metric)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train/Test"
      ],
      "metadata": {
        "id": "zMZgHjBaoaBK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CzWRcoH4AH0"
      },
      "source": [
        "### - Set Up Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb0-VAbVNgEr"
      },
      "outputs": [],
      "source": [
        "\"\"\" Create paths \"\"\"\n",
        "GCP_PROJECT_ID = 'model-training-383203'\n",
        "GCS_BUCKET  = 'model_sum'\n",
        "REGION = 'us-central1'\n",
        "JOB_NAME = f'model_{co_model}'\n",
        "AUTH_JSON = '/content/model-training-383203-38e4420de909.json'\n",
        "REQUIRE = '/content/model-require.txt'\n",
        "co_model += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow9Urv39FC8D"
      },
      "outputs": [],
      "source": [
        "\"\"\" Define storage paths \"\"\"\n",
        "GCS_BASE_PATH = f'gs://{GCS_BUCKET}/{JOB_NAME}'\n",
        "TENSORBOARD_LOGS = os.path.join(GCS_BASE_PATH,\"logs\")\n",
        "MODEL_CP = os.path.join(GCS_BASE_PATH,\"checkpoints\")\n",
        "SAVED_MODEL_DIR = os.path.join(GCS_BASE_PATH,\"saved_model\")\n",
        "TOKENIZE_DIR = os.path.join(GCS_BASE_PATH, 'tokenizer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBsmTvfutjay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d349da7-1ad5-44d9-950c-cdf83ae4df51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=9OWwLsMBg76F9UftSml4wPbBizUR1G&prompt=consent&access_type=offline&code_challenge=oyZZhbfIg5lkXMrQHtte249uMuO3hHRWRL6uQOPsgVo&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AVHEtk7of_n1yEbRpxhZGR3yVTLQ60jF1o4rY6YkveMGW3SVPscZBvLxo82jVa--R7BvIQ\n",
            "\n",
            "You are now logged in as [jetheat2002@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n",
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Authorize user and Set storage paths \"\"\"\n",
        "if not tfc.remote() and (\"google.colab\" in sys.modules):\n",
        "  if not pre_auth:\n",
        "    !gcloud auth login\n",
        "    !gcloud config set project 136963608278\n",
        "    auth.authenticate_user()\n",
        "    pre_auth = True\n",
        "\n",
        "  if pre_auth:\n",
        "    os.environ[\"GOOGLE_CLOUD_PROJECT\"] = GCP_PROJECT_ID\n",
        "    os.environ[\"GCS_BUCKET\"] = GCS_BUCKET\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = AUTH_JSON\n",
        "    os.environ['REGION'] = REGION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - Train Model"
      ],
      "metadata": {
        "id": "dFYSC11Zi3Sl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKYtId77R4D4"
      },
      "outputs": [],
      "source": [
        "\"\"\" Get model \"\"\"\n",
        "model = get_model()\n",
        "compile_model(model, rm_metric='avg')\n",
        "callbacks = get_callbacks(TENSORBOARD_LOGS, MODEL_CP, es_patience=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, to_file='model_arch.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "kbpFQ0ItZRgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Define hyper-parameters\"\"\"\n",
        "if tfc.remote():\n",
        "  val_split = 0.20\n",
        "  num_batch = 32\n",
        "  num_epoch = 1024\n",
        "else:\n",
        "  val_split = 0.15\n",
        "  num_batch = 8\n",
        "  num_epoch = 250"
      ],
      "metadata": {
        "id": "bzyfxuceQ2WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZj2JuCs6-It"
      },
      "outputs": [],
      "source": [
        "\"\"\" Train model \"\"\"\n",
        "history = model.fit([x_train, y_train[:,:-1]], y_train[:,1:],\n",
        "                     validation_split=val_split,\n",
        "                     batch_size=num_batch,\n",
        "                     epochs=num_epoch,\n",
        "                     callbacks=callbacks,\n",
        "                     verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Save extra model data \"\"\"\n",
        "model.save(SAVED_MODEL_DIR)\n",
        "\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(GCS_BUCKET)\n",
        "blob = bucket.blob(TOKENIZE_DIR)\n",
        "token_json = tokenizer.to_json()\n",
        "\n",
        "with open('tokenizer.json', 'w') as f:\n",
        "  f.write(token_json)\n",
        "\n",
        "blob.upload_from_filename('tokenizer.json')"
      ],
      "metadata": {
        "id": "O8QeGvUNbGHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# docker = DockerConfig(image_build_bucket=GCS_BUCKET)\n",
        "# # entry_point = ...\n",
        "# tfc.run(\n",
        "#         requirements_txt=REQUIRE,\n",
        "#         distribution_strategy=\"auto\",\n",
        "#         docker_config=docker\n",
        "# )"
      ],
      "metadata": {
        "id": "KmVDNw5bPw8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2IAufRfY819"
      },
      "outputs": [],
      "source": [
        "# \"\"\" Uploading \"\"\"\n",
        "# storage_client = storage.Client()\n",
        "# bucket = storage_client.bucket(GCS_BUCKET)\n",
        "# blob = bucket.blob(TOKENIZE_DIR)\n",
        "# token_json = tokenizer.to_json()\n",
        "\n",
        "# with open('tokenizer.json', 'w') as f:\n",
        "#   f.write(token_json)\n",
        "\n",
        "# blob.upload_from_filename('tokenizer.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIO8QkR0XBwy"
      },
      "outputs": [],
      "source": [
        "# \"\"\" Downloading \"\"\"\n",
        "# storage_client = storage.Client()\n",
        "# bucket = storage_client.bucket(GCS_BUCKET)\n",
        "# blob = bucket.blob(TOKENIZE_DIR)\n",
        "# blob.download_to_filename('tokenizer.json')\n",
        "\n",
        "# with open('tokenizer.json', 'r') as f:\n",
        "#   token_json = f.read()\n",
        "#   test = tokenizer_from_json(token_json)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "EXamseOBiRnz",
        "dr5Ja47mNUIZ",
        "rFcdMS9voycV",
        "0EI4TrajcxEr",
        "DyZafR4RfufJ",
        "RevvLw3IbyiD",
        "_gqHbB2OoKjz",
        "E2YrkK0mM0W2",
        "f6NvVO4qbCqs",
        "8CzWRcoH4AH0",
        "dFYSC11Zi3Sl",
        "jbLmmDglsBVa",
        "2zM7oVHY4Krb"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}