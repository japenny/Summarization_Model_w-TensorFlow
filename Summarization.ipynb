{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbbUOZQuVm7V"
      },
      "source": [
        "# Abstractive Text Summarization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXamseOBiRnz"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtEIWIDAzGKh"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow-cloud\n",
        "!pip install tensorflow_cloud\n",
        "!pip install tensorflow-addons\n",
        "!pip install keras-nlp\n",
        "!pip install rouge-score\n",
        "!pip install rouge\n",
        "!pip install tf-nightly\n",
        "!pip install pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b028gw4xSNYL"
      },
      "outputs": [],
      "source": [
        "\"\"\" Datasets \"\"\"\n",
        "import pyarrow\n",
        "from datasets import load_dataset, load_dataset_builder\n",
        "# import gensim.downloader as api\n",
        "\n",
        "\"\"\" Building model \"\"\"\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from tensorflow.keras.layers import Attention, Bidirectional, Concatenate, Dense, Embedding, Flatten, Input, LayerNormalization, LSTM, MultiHeadAttention\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
        "from tensorflow.keras.regularizers import *\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "\"\"\" Training/Testing model \"\"\"\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.initializers import Zeros\n",
        "from tensorflow.keras.metrics import Metric, F1Score\n",
        "# from keras_nlp.metrics import RougeL\n",
        "from rouge_score import rouge_scorer as rs\n",
        "# from rouge import Rouge\n",
        "# from tensorflow_addons.seq2seq import BeamSearchDecoder\n",
        "# from keras_nlp.utils import beam_search\n",
        "\n",
        "\"\"\" TF Cloud Training \"\"\"\n",
        "import tensorflow_cloud as tfc\n",
        "from tensorflow_cloud.core.docker_config import DockerConfig\n",
        "\n",
        "\"\"\" Data processing/visualization \"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\"\"\" Cloud \"\"\"\n",
        "from google.colab import auth, files\n",
        "from google.cloud import storage\n",
        "\n",
        "\"\"\" Other \"\"\"\n",
        "import sys\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr5Ja47mNUIZ"
      },
      "source": [
        "## Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4GrqJCRbOvs"
      },
      "outputs": [],
      "source": [
        "\"\"\" Check Data \"\"\"\n",
        "ds_name = 'cnn_dailymail' # 'GEM/wiki_lingua' 'ccdv/pubmed-summarization' 'scientific_papers'\n",
        "ds_sub = '3.0.0'\n",
        "builder = load_dataset_builder(ds_name, ds_sub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3IO0630B7Ut"
      },
      "outputs": [],
      "source": [
        "\"\"\" Check Data cont. \"\"\"\n",
        "print(builder.info.description)\n",
        "builder.info.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLhN-tKz3eeu"
      },
      "outputs": [],
      "source": [
        "\"\"\" Load Dataset \"\"\"\n",
        "dataset = load_dataset(ds_name, ds_sub)\n",
        "\n",
        "split_train = len(dataset['train']['article'])\n",
        "split_val = len(dataset['validation']['article'])\n",
        "\n",
        "if not tfc.remote():\n",
        "  split_train = 1000\n",
        "  split_val = 1000\n",
        "\n",
        "st = 30\n",
        "pre_auth = False\n",
        "co_model = 7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preperation"
      ],
      "metadata": {
        "id": "rFcdMS9voycV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - Split Data"
      ],
      "metadata": {
        "id": "0EI4TrajcxEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gdw8nQ-IKS_c"
      },
      "outputs": [],
      "source": [
        "\"\"\" Split Data and add sos/eos tokens \"\"\"\n",
        "sos_token = '<sos>'\n",
        "eos_token = '<eos>'\n",
        "\n",
        "x_train = np.array([f'{sos_token} {art} {eos_token}' for art in dataset['train']['article'][:split_train]])\n",
        "time.sleep(st)\n",
        "y_train = np.array([f'{sos_token} {sum} {eos_token}' for sum in dataset['train']['highlights'][:split_train]])\n",
        "time.sleep(st)\n",
        "\n",
        "x_val = np.array([f'{sos_token} {art} {eos_token}' for art in dataset['validation']['article'][:split_val]])\n",
        "y_val = np.array([f'{sos_token} {sum} {eos_token}' for sum in dataset['validation']['highlights'][:split_val]])\n",
        "\n",
        "time.sleep(st)\n",
        "x_train = np.concatenate([x_train, x_val], axis=0)\n",
        "time.sleep(st)\n",
        "y_train = np.concatenate([y_train, y_val], axis=0)\n",
        "\n",
        "del x_val, y_val\n",
        "\n",
        "x_test = np.array([f'{sos_token} {art} {eos_token}' for art in dataset['test']['article']])\n",
        "y_test = np.array([f'{sos_token} {sum} {eos_token}' for sum in dataset['test']['highlights']])\n",
        "\n",
        "print(f'x_train shape: {x_train.shape}, y_train shape: {y_train.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnGHfGZpKnn4"
      },
      "outputs": [],
      "source": [
        "\"\"\" Data Samples \"\"\"\n",
        "df = pd.DataFrame({'Article':x_train[:5], 'Summary':y_train[:5]})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td7fhHRbZwQt"
      },
      "outputs": [],
      "source": [
        "del dataset, df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyZafR4RfufJ"
      },
      "source": [
        "### - Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQkT_FGdftGG"
      },
      "outputs": [],
      "source": [
        "### optimize preprocessing ###\n",
        "\"\"\"\n",
        "tokenize data first\n",
        "split data\n",
        "\n",
        "get all stop word indexes from tokenizer.word_index\n",
        "remove stop stop words??\n",
        "\n",
        "pad data as normal\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhAueUfbflCS"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['cnn', 'reuters'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrDwxDFwfzuw"
      },
      "outputs": [],
      "source": [
        "\"\"\" Lowercase all words \"\"\"\n",
        "x_train, y_train = np.char.lower(x_train), np.char.lower(y_train)\n",
        "x_test, y_test = np.char.lower(x_test), np.char.lower(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOA-p_ybf2tD"
      },
      "outputs": [],
      "source": [
        "\"\"\" Remove stop words \"\"\"\n",
        "pattern = re.compile(\"\\\\b(\" + \"|\".join(stop_words) + \")\\\\b\")\n",
        "\n",
        "vec_pattern = np.vectorize(lambda text:pattern.sub('', text))\n",
        "time.sleep(st)\n",
        "x_train = vec_pattern(x_train)\n",
        "time.sleep(st)\n",
        "y_train = vec_pattern(y_train)\n",
        "\n",
        "del pattern, stop_words, vec_pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - Transform Data"
      ],
      "metadata": {
        "id": "RevvLw3IbyiD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdS-HHpFgAGp"
      },
      "outputs": [],
      "source": [
        "\"\"\" Tokenize data \"\"\"\n",
        "tokenizer = Tokenizer(filters='\"#$%&(!?.)\\'*+,-/:;=@[\\\\]^_`{|}~\\t\\n')#, oov_token=\"<unk>\")\n",
        "\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "tokenizer.fit_on_texts(y_train)\n",
        "\n",
        "x_train, y_train = tokenizer.texts_to_sequences(x_train), tokenizer.texts_to_sequences(y_train)\n",
        "x_test, y_test = tokenizer.texts_to_sequences(x_test), tokenizer.texts_to_sequences(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqEzkUGnbS0F"
      },
      "outputs": [],
      "source": [
        "\"\"\" Pad tokenized data \"\"\"\n",
        "x_train, y_train = pad_sequences(x_train, padding='post'), pad_sequences(y_train, padding='post')\n",
        "x_test, y_test = pad_sequences(x_test, padding='post'), pad_sequences(y_test, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLx5AJZzbXcj"
      },
      "outputs": [],
      "source": [
        "\"\"\" After Processing Data Samples \"\"\"\n",
        "df = pd.DataFrame(x_train[:3])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g8bf808-pQ-"
      },
      "outputs": [],
      "source": [
        "del df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model\n"
      ],
      "metadata": {
        "id": "_gqHbB2OoKjz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2YrkK0mM0W2"
      },
      "source": [
        "### - Develop Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Load Pre-Trained Word Embeddings \"\"\"\n",
        "w2v_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCt3YEPv374K",
        "outputId": "fb264edf-a0f2-417f-bff3-872fb0898eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7PqZMOf6JVz"
      },
      "outputs": [],
      "source": [
        "\"\"\" Create Embedding Matrix \"\"\"\n",
        "vocab_dim = len(tokenizer.word_index)+1\n",
        "emb_dim = 300\n",
        "x_row, x_col = x_train.shape\n",
        "y_row, y_col = y_train.shape\n",
        "\n",
        "emb_matrix = np.zeros((vocab_dim, emb_dim))\n",
        "\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "  if word in w2v_model:\n",
        "    emb_matrix[idx] = w2v_model[word]\n",
        "\n",
        "del w2v_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(lat_0=128, lat_1=128,\n",
        "                  dr_0=0.35, dr_1=0.35, dr_2=0.35,\n",
        "                  att_0=2, att_1=2,\n",
        "                  beam_width=4,\n",
        "                  vocab_len=vocab_dim, emb_len=emb_dim):\n",
        "\n",
        "  # Encoder\n",
        "  encoder_input     = Input(shape=(None,), name='Input_0')\n",
        "\n",
        "  encoder_emb_layer = Embedding(weights=[emb_matrix], mask_zero=True, trainable=False, name='Embedding_0')\n",
        "  encoder_emb_input = encoder_emb_layer(encoder_input)\n",
        "\n",
        "  encoder_lstm_0    = Bidirectional(LSTM(units=lat_0, dropout=dr_0, return_sequences=True), name='Bidirectional_LSTM_0')\n",
        "  encoder_lstm_out  = encoder_lstm_0(encoder_emb_input)\n",
        "\n",
        "  encoder_lay_norm  = LayerNormalization(name='Layer_Norm_0')\n",
        "  encoder_lstm_norm = encoder_lay_norm(encoder_lstm_out)\n",
        "\n",
        "  encoder_self_att  = MultiHeadAttention(num_heads=att_0, key_dim=emb_len, query_key_shared=True, name='Attention_0')\n",
        "  encoder_att_out   = encoder_self_att(encoder_lstm_norm)\n",
        "\n",
        "  encoder_lstm_1    = Bidirectional(LSTM(units=lat_1, dropout=dr_1, return_sequences=True, return_state=True), name='Bidirectional_LSTM_1')\n",
        "  encoder_out, forward_h, forward_c, backward_h, backward_c = encoder_lstm_1(encoder_att_out)\n",
        "\n",
        "  forward_h_concat  = Concatenate(name='Concat_0')([forward_h, backward_h])\n",
        "  forward_c_concat  = Concatenate(name='Concat_1')([forward_c, backward_c])\n",
        "  encoder_states    = [forward_h_concat, forward_c_concat]\n",
        "\n",
        "\n",
        "  # Decoder\n",
        "  decoder_input     = Input(shape=(None,), name='Input_1')\n",
        "\n",
        "  decoder_emb_layer = Embedding(weights=[emb_matrix], mask_zero=True, trainable=False, name='Embedding_1')\n",
        "  decoder_emb_out   = decoder_emb_layer(decoder_input)\n",
        "\n",
        "  decoder_lstm      = LSTM(units=lat_1*2, dropout=dr_2, return_sequences=True, name='LSTM_0')\n",
        "  decoder_lstm_out  = decoder_lstm(decoder_emb_out, initial_state=encoder_states)\n",
        "\n",
        "  decoder_lay_norm  = LayerNormalization(name='Layer_Norm_1')\n",
        "  decoder_lstm_norm = decoder_lay_norm(decoder_lstm_out)\n",
        "\n",
        "  decoder_multi_att = MultiHeadAttention(num_heads=att_1, key_dim=emb_len, name='Attention_1')\n",
        "  decoder_att_out   = decoder_multi_att(query=decoder_lstm_norm, key=encoder_out)\n",
        "\n",
        "  decoder_dense     = Dense(vocab_len, activation='softmax', name='Dense_0')\n",
        "  decoder_dense_out = decoder_dense(decoder_att_out)\n",
        "\n",
        "  # Create model\n",
        "  model = Model([encoder_input.input, decoder_input(encoder_input.output)], decoder_dense_out, name='Text_Summarization_Model')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "VMz9j-YsRs1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - Develop Metrics/Callbacks"
      ],
      "metadata": {
        "id": "f6NvVO4qbCqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Create rouge score metric \"\"\"\n",
        "class RougeMetric(Metric):\n",
        "\n",
        "  def __init__(self, method='avg'):\n",
        "    super().__init__(name='f1_rs')\n",
        "\n",
        "    if method not in {'avg', 'min', 'max'}:\n",
        "      raise ValueError(\"Invalid score method, expected 'min', 'avg' or 'max' (str)\")\n",
        "    self.method = method\n",
        "    self.rouge_scoring = rs.RougeScorer(['rougeL'])\n",
        "\n",
        "    if self.method == 'min':\n",
        "      self.f1_score = tf.Variable(1.0, dtype=tf.float32, trainable=False)\n",
        "    else:\n",
        "      self.f1_score = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
        "    self.co = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
        "\n",
        "  def sequences_to_texts(self, sequence): # (47, ) -> (1, )\n",
        "      return tokenizer.sequences_to_texts(sequence.numpy().reshape(1, -1))\n",
        "\n",
        "  def tf_sequences_to_texts(self, sequence): # (47, ) -> (1, )\n",
        "      return tf.py_function(self.sequences_to_texts, [sequence], tf.string)\n",
        "\n",
        "  def get_f1(self, ref, hyp): # (2, ) -> int\n",
        "      score = self.rouge_scoring.score(ref.numpy(), hyp.numpy())\n",
        "      return score['rougeL'].fmeasure\n",
        "\n",
        "  def get_rouge(self, vals): # (2, ) -> int\n",
        "      return tf.py_function(self.get_f1, [vals[0], vals[1]], tf.float32)\n",
        "\n",
        "  def update_state(self, y_true, y_preds, sample_weight=None):\n",
        "    max_preds = tf.convert_to_tensor(tf.argmax(y_preds, axis=-1)) # (50, 47)\n",
        "\n",
        "    text_preds = tf.map_fn(self.tf_sequences_to_texts, max_preds, dtype=tf.string) # (50, 47) -> (50, )\n",
        "    text_true = tf.map_fn(self.tf_sequences_to_texts, y_true, dtype=tf.string) # (50, 47) -> (50, )\n",
        "\n",
        "    scores_f1 = tf.map_fn(self.get_rouge, (text_true, text_preds), dtype=tf.float32) # (50, 50) -> (50, )\n",
        "\n",
        "    if self.method == 'min':\n",
        "      self.f1_score.assign(tf.minimum(self.f1_score, tf.reduce_min(scores_f1)))\n",
        "    elif self.method == 'avg':\n",
        "      self.f1_score.assign_add(tf.reduce_sum(scores_f1))\n",
        "      self.co.assign_add(tf.cast(tf.shape(y_true)[0], dtype=tf.float32))\n",
        "    elif self.method == 'max':\n",
        "      self.f1_score.assign(tf.maximum(self.f1_score, tf.reduce_max(scores_f1)))\n",
        "    else:\n",
        "      raise ValueError(\"Invalid score method when updating f1_score, expected 'min', 'avg' or 'max' (str)\")\n",
        "\n",
        "  def result(self):\n",
        "    if self.method == 'avg':\n",
        "      avg = self.f1_score / self.co\n",
        "      return tf.round(avg * 10_000) / 10_000\n",
        "    else:\n",
        "      return tf.round(self.f1_score * 10_000) / 10_000\n",
        "\n",
        "  def reset_state(self):\n",
        "    if self.method == 'min':\n",
        "        self.f1_score.assign(1.0)\n",
        "    else:\n",
        "        self.f1_score.assign(0.0)\n",
        "    self.co.assign(0.0)\n"
      ],
      "metadata": {
        "id": "ujZp9Fv5RbH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUziV7LIblNr"
      },
      "outputs": [],
      "source": [
        "\"\"\" Create Callbacks \"\"\"\n",
        "def get_callbacks(tb_path, cp_path,\n",
        "                  rlr_factor=0.1, rlr_patience=3,\n",
        "                  es_patience=6):\n",
        "                  #val_loss, val_f1_rs\n",
        "  early_stop   = EarlyStopping(monitor='loss',\n",
        "                               patience=es_patience)\n",
        "\n",
        "  reduce_lr    = ReduceLROnPlateau(factor=rlr_factor,\n",
        "                                   patience=rlr_patience)\n",
        "\n",
        "  tensor_board = TensorBoard(log_dir=tb_path)\n",
        "\n",
        "  model_cp     = ModelCheckpoint(filepath=cp_path,\n",
        "                                 monitor='loss',\n",
        "                                 save_best_only=True,\n",
        "                                 save_freq='epoch',\n",
        "                                 verbose=1)\n",
        "\n",
        "  return early_stop, model_cp, tensor_board#, reduce_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vp_7sh8bpiH"
      },
      "outputs": [],
      "source": [
        "\"\"\" Compile Model \"\"\"\n",
        "def compile_model(model, lr=0.001, rm_metric='avg'):\n",
        "\n",
        "  model.compile(optimizer=Adam(lr),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=[RougeMetric(rm_metric)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training\n"
      ],
      "metadata": {
        "id": "zMZgHjBaoaBK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CzWRcoH4AH0"
      },
      "source": [
        "### - Set Up Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb0-VAbVNgEr"
      },
      "outputs": [],
      "source": [
        "\"\"\" Create paths \"\"\"\n",
        "GCP_PROJECT_ID = 'model-training-383203'\n",
        "GCS_BUCKET  = 'model_sum'\n",
        "REGION = 'us-central1'\n",
        "JOB_NAME = f'model_{co_model}'\n",
        "AUTH_JSON = '/content/model-training-383203-38e4420de909.json'\n",
        "REQUIRE = '/content/model-require.txt'\n",
        "co_model += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow9Urv39FC8D"
      },
      "outputs": [],
      "source": [
        "\"\"\" Define storage paths \"\"\"\n",
        "GCS_BASE_PATH = f'gs://{GCS_BUCKET}/{JOB_NAME}'\n",
        "TENSORBOARD_LOGS = os.path.join(GCS_BASE_PATH,\"logs\")\n",
        "MODEL_CP = os.path.join(GCS_BASE_PATH,\"checkpoints\")\n",
        "SAVED_MODEL_DIR = os.path.join(GCS_BASE_PATH,\"saved_model\")\n",
        "TOKENIZE_DIR = os.path.join(GCS_BASE_PATH, 'tokenizer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBsmTvfutjay"
      },
      "outputs": [],
      "source": [
        "\"\"\" Authorize user and Set storage paths \"\"\"\n",
        "if not tfc.remote() and (\"google.colab\" in sys.modules):\n",
        "  if not pre_auth:\n",
        "    !gcloud auth login\n",
        "    !gcloud config set project 136963608278\n",
        "    auth.authenticate_user()\n",
        "    pre_auth = True\n",
        "\n",
        "  if pre_auth:\n",
        "    os.environ[\"GOOGLE_CLOUD_PROJECT\"] = GCP_PROJECT_ID\n",
        "    os.environ[\"GCS_BUCKET\"] = GCS_BUCKET\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = AUTH_JSON\n",
        "    os.environ['REGION'] = REGION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - Train Model"
      ],
      "metadata": {
        "id": "dFYSC11Zi3Sl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKYtId77R4D4"
      },
      "outputs": [],
      "source": [
        "\"\"\" Get model \"\"\"\n",
        "model = get_model()\n",
        "compile_model(model, rm_metric='avg')\n",
        "callbacks = get_callbacks(TENSORBOARD_LOGS, MODEL_CP, es_patience=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, to_file='model_arch.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "kbpFQ0ItZRgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Define hyper-parameters\"\"\"\n",
        "if tfc.remote():\n",
        "  val_split = 0.20\n",
        "  num_batch = 32\n",
        "  num_epoch = 1024\n",
        "else:\n",
        "  val_split = 0.15\n",
        "  num_batch = 8\n",
        "  num_epoch = 250"
      ],
      "metadata": {
        "id": "bzyfxuceQ2WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZj2JuCs6-It"
      },
      "outputs": [],
      "source": [
        "\"\"\" Train model \"\"\"\n",
        "history = model.fit([x_train, y_train[:,:-1]], y_train[:,1:],\n",
        "                     validation_split=val_split,\n",
        "                     batch_size=num_batch,\n",
        "                     epochs=num_epoch,\n",
        "                     callbacks=callbacks,\n",
        "                     verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Save extra model data \"\"\"\n",
        "model.save(SAVED_MODEL_DIR)\n",
        "\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(GCS_BUCKET)\n",
        "blob = bucket.blob(TOKENIZE_DIR)\n",
        "token_json = tokenizer.to_json()\n",
        "\n",
        "with open('tokenizer.json', 'w') as f:\n",
        "  f.write(token_json)\n",
        "\n",
        "blob.upload_from_filename('tokenizer.json')"
      ],
      "metadata": {
        "id": "O8QeGvUNbGHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# docker = DockerConfig(image_build_bucket=GCS_BUCKET)\n",
        "# # entry_point = ...\n",
        "# tfc.run(\n",
        "#         requirements_txt=REQUIRE,\n",
        "#         distribution_strategy=\"auto\",\n",
        "#         docker_config=docker\n",
        "# )"
      ],
      "metadata": {
        "id": "KmVDNw5bPw8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2IAufRfY819"
      },
      "outputs": [],
      "source": [
        "# \"\"\" Uploading \"\"\"\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(GCS_BUCKET)\n",
        "blob = bucket.blob(TOKENIZE_DIR)\n",
        "token_json = tokenizer.to_json()\n",
        "\n",
        "with open('tokenizer.json', 'w') as f:\n",
        "  f.write(token_json)\n",
        "\n",
        "blob.upload_from_filename('tokenizer.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIO8QkR0XBwy"
      },
      "outputs": [],
      "source": [
        "# \"\"\" Downloading \"\"\"\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(GCS_BUCKET)\n",
        "blob = bucket.blob(TOKENIZE_DIR)\n",
        "blob.download_to_filename('tokenizer.json')\n",
        "\n",
        "with open('tokenizer.json', 'r') as f:\n",
        "  token_json = f.read()\n",
        "  test = tokenizer_from_json(token_json)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "EXamseOBiRnz",
        "dr5Ja47mNUIZ",
        "rFcdMS9voycV",
        "0EI4TrajcxEr",
        "DyZafR4RfufJ",
        "RevvLw3IbyiD",
        "_gqHbB2OoKjz",
        "E2YrkK0mM0W2",
        "f6NvVO4qbCqs",
        "zMZgHjBaoaBK",
        "8CzWRcoH4AH0",
        "dFYSC11Zi3Sl"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}